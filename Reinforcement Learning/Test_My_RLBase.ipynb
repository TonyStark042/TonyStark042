{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5f0283d1-9b44-4c4c-bdac-9edba9089e09",
   "metadata": {},
   "outputs": [],
   "source": [
    "from My_RLBase import REINFORCE\n",
    "import gymnasium as gym\n",
    "import torch\n",
    "from My_RLBase import *\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9e29e811-7489-4e18-af4d-795ec7b4d0bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bd9c6a19-fba4-49a4-81de-a354724647b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e1b6e200-64ba-4fb4-a13b-31b84225c2f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "env_id = \"CartPole-v1\"    # flappy bird env id\n",
    "\n",
    "env = gym.make(env_id, render_mode='rgb_array')      # creating the env\n",
    "env.reset( )\n",
    "\n",
    "s_size = env.observation_space.shape[0]     # observation space\n",
    "a_size = env.action_space.n                 # action space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "fb57e896-3cb6-4f92-afa2-15e8e1f14b92",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = REINFORCE(s_size = s_size, a_size = a_size, h_size = 32, epochs=500, max_steps=500, tail_op=True, lr=3e-3, tail=-1.5,gamma=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "43c4f3fc-d35d-4f14-9937-c0ff589bf866",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 10\tAverage Score: 19.1\tPolicy Loss: 29.194292068481445\n",
      "==========================Best_agent has changed, the best reward is 19.1======================\n",
      "Episode: 20\tAverage Score: 14.1\tPolicy Loss: 66.58484649658203\n",
      "Episode: 30\tAverage Score: 26.9\tPolicy Loss: 82.78962707519531\n",
      "==========================Best_agent has changed, the best reward is 26.9======================\n",
      "Episode: 40\tAverage Score: 32.4\tPolicy Loss: 139.2040252685547\n",
      "==========================Best_agent has changed, the best reward is 32.4======================\n",
      "Episode: 50\tAverage Score: 28.2\tPolicy Loss: 30.853294372558594\n",
      "Episode: 60\tAverage Score: 23.8\tPolicy Loss: 37.65592575073242\n",
      "Episode: 70\tAverage Score: 31.5\tPolicy Loss: 119.55547332763672\n",
      "Episode: 80\tAverage Score: 52.4\tPolicy Loss: 446.8948669433594\n",
      "==========================Best_agent has changed, the best reward is 52.4======================\n",
      "Episode: 90\tAverage Score: 49.1\tPolicy Loss: 196.27847290039062\n",
      "Episode: 100\tAverage Score: 50.0\tPolicy Loss: 442.3887634277344\n",
      "Episode: 110\tAverage Score: 81.5\tPolicy Loss: 153.59844970703125\n",
      "==========================Best_agent has changed, the best reward is 81.5======================\n",
      "Episode: 120\tAverage Score: 81.7\tPolicy Loss: 114.04702758789062\n",
      "==========================Best_agent has changed, the best reward is 81.7======================\n",
      "Episode: 130\tAverage Score: 56.7\tPolicy Loss: 153.96026611328125\n",
      "Episode: 140\tAverage Score: 59.7\tPolicy Loss: 50.75350570678711\n",
      "Episode: 150\tAverage Score: 43.7\tPolicy Loss: 578.2498779296875\n",
      "Episode: 160\tAverage Score: 41.8\tPolicy Loss: 563.254638671875\n",
      "Episode: 170\tAverage Score: 169.55\tPolicy Loss: 760.0172119140625\n",
      "==========================Best_agent has changed, the best reward is 169.55======================\n",
      "Episode: 180\tAverage Score: 252.2\tPolicy Loss: 2820.093017578125\n",
      "==========================Best_agent has changed, the best reward is 252.2======================\n",
      "Episode: 190\tAverage Score: 336.9\tPolicy Loss: 2126.3076171875\n",
      "==========================Best_agent has changed, the best reward is 336.9======================\n",
      "Episode: 200\tAverage Score: 213.55\tPolicy Loss: 141.83290100097656\n",
      "Episode: 210\tAverage Score: 63.5\tPolicy Loss: 537.1341552734375\n",
      "Episode: 220\tAverage Score: 62.2\tPolicy Loss: 90.85345458984375\n",
      "Episode: 230\tAverage Score: 239.65\tPolicy Loss: 2711.32763671875\n",
      "Episode: 240\tAverage Score: 447.2\tPolicy Loss: 2296.33349609375\n",
      "==========================Best_agent has changed, the best reward is 447.2======================\n",
      "Episode: 250\tAverage Score: 421.75\tPolicy Loss: 2693.2431640625\n",
      "Episode: 260\tAverage Score: 468.85\tPolicy Loss: 2819.665283203125\n",
      "==========================Best_agent has changed, the best reward is 468.85======================\n",
      "Episode: 270\tAverage Score: 482.7\tPolicy Loss: 2152.39306640625\n",
      "==========================Best_agent has changed, the best reward is 482.7======================\n",
      "Episode: 280\tAverage Score: 168.5\tPolicy Loss: 148.5336456298828\n",
      "Episode: 290\tAverage Score: 26.8\tPolicy Loss: 57.55274200439453\n",
      "Episode: 300\tAverage Score: 22.8\tPolicy Loss: 110.28992462158203\n",
      "Episode: 310\tAverage Score: 24.0\tPolicy Loss: 60.781280517578125\n",
      "Episode: 320\tAverage Score: 39.4\tPolicy Loss: 157.45777893066406\n",
      "Episode: 330\tAverage Score: 66.8\tPolicy Loss: 376.44281005859375\n",
      "Episode: 340\tAverage Score: 94.5\tPolicy Loss: 649.336181640625\n",
      "Episode: 350\tAverage Score: 127.9\tPolicy Loss: 712.3076171875\n",
      "Episode: 360\tAverage Score: 139.3\tPolicy Loss: 695.5625\n",
      "Episode: 370\tAverage Score: 150.1\tPolicy Loss: 621.4718017578125\n",
      "Episode: 380\tAverage Score: 181.2\tPolicy Loss: 1251.997314453125\n",
      "Episode: 390\tAverage Score: 238.0\tPolicy Loss: 1141.3482666015625\n",
      "Episode: 400\tAverage Score: 184.9\tPolicy Loss: 744.1600341796875\n",
      "Episode: 410\tAverage Score: 91.4\tPolicy Loss: 464.30023193359375\n",
      "Episode: 420\tAverage Score: 131.2\tPolicy Loss: 966.0294189453125\n",
      "Episode: 430\tAverage Score: 204.3\tPolicy Loss: 1407.126953125\n",
      "Episode: 440\tAverage Score: 337.7\tPolicy Loss: 1601.1409912109375\n",
      "Episode: 450\tAverage Score: 242.3\tPolicy Loss: 939.1299438476562\n",
      "Episode: 460\tAverage Score: 142.3\tPolicy Loss: 596.0746459960938\n",
      "Episode: 470\tAverage Score: 70.8\tPolicy Loss: 391.9204406738281\n",
      "Episode: 480\tAverage Score: 74.6\tPolicy Loss: 47.06695556640625\n",
      "Episode: 490\tAverage Score: 108.4\tPolicy Loss: 516.0225219726562\n",
      "Episode: 500\tAverage Score: 95.4\tPolicy Loss: 453.51678466796875\n"
     ]
    }
   ],
   "source": [
    "best, rewards, losses = Train_REINFORCE(env, agent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "276628e8-b7f2-48ea-ad63-39d93da30cec",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "开始测试！\n",
      "回合：1/20，奖励：500.00\n",
      "回合：2/20，奖励：500.00\n",
      "回合：3/20，奖励：500.00\n",
      "回合：4/20，奖励：352.00\n",
      "回合：5/20，奖励：400.00\n",
      "回合：6/20，奖励：500.00\n",
      "回合：7/20，奖励：461.00\n",
      "回合：8/20，奖励：475.00\n",
      "回合：9/20，奖励：500.00\n",
      "回合：10/20，奖励：500.00\n",
      "回合：11/20，奖励：498.00\n",
      "回合：12/20，奖励：434.00\n",
      "回合：13/20，奖励：331.00\n",
      "回合：14/20，奖励：500.00\n",
      "回合：15/20，奖励：381.00\n",
      "回合：16/20，奖励：500.00\n",
      "回合：17/20，奖励：338.00\n",
      "回合：18/20，奖励：435.00\n",
      "回合：19/20，奖励：428.00\n",
      "回合：20/20，奖励：416.00\n",
      "完成测试\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'rewards': [500.0,\n",
       "  500.0,\n",
       "  500.0,\n",
       "  352.0,\n",
       "  400.0,\n",
       "  500.0,\n",
       "  461.0,\n",
       "  475.0,\n",
       "  500.0,\n",
       "  500.0,\n",
       "  498.0,\n",
       "  434.0,\n",
       "  331.0,\n",
       "  500.0,\n",
       "  381.0,\n",
       "  500.0,\n",
       "  338.0,\n",
       "  435.0,\n",
       "  428.0,\n",
       "  416.0]}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test(env,best,20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e4db6a8a-d69d-4cdf-b285-d42b246ab4df",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_2 = PPO(s_size, a_size, lr_actor = 3e-3, lr_critic = 3e-4, gamma = 0.99, epochs = 300, max_steps = 500, update_times = 20,  update_steps = 2000, eps_clip = 0.2, print_freq = 10, has_continuous_action_space = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "19231d76-cc1b-4e98-b6d8-f51f387b67f7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started training at (GMT) :  2023-11-28 23:56:22\n",
      "============================================================================================\n",
      "Episode : 10 \t\t Timestep : 153 \t\t Average Reward : 15.11\n",
      "==========================Best_agent has changed, the best reward is 15.11======================\n",
      "Episode : 20 \t\t Timestep : 347 \t\t Average Reward : 19.2\n",
      "==========================Best_agent has changed, the best reward is 19.2======================\n",
      "Episode : 30 \t\t Timestep : 529 \t\t Average Reward : 18.5\n",
      "Episode : 40 \t\t Timestep : 728 \t\t Average Reward : 19.8\n",
      "==========================Best_agent has changed, the best reward is 19.8======================\n",
      "Episode : 50 \t\t Timestep : 899 \t\t Average Reward : 17.3\n",
      "Episode : 60 \t\t Timestep : 1119 \t\t Average Reward : 18.7\n",
      "Episode : 70 \t\t Timestep : 1307 \t\t Average Reward : 20.7\n",
      "==========================Best_agent has changed, the best reward is 20.7======================\n",
      "Episode : 80 \t\t Timestep : 1494 \t\t Average Reward : 20.6\n",
      "Episode : 90 \t\t Timestep : 1686 \t\t Average Reward : 18.8\n",
      "Episode : 100 \t\t Timestep : 1898 \t\t Average Reward : 21.0\n",
      "==========================Best_agent has changed, the best reward is 21.0======================\n",
      "Episode : 110 \t\t Timestep : 2084 \t\t Average Reward : 15.5\n",
      "Episode : 120 \t\t Timestep : 2392 \t\t Average Reward : 30.5\n",
      "==========================Best_agent has changed, the best reward is 30.5======================\n",
      "Episode : 130 \t\t Timestep : 2649 \t\t Average Reward : 28.7\n",
      "Episode : 140 \t\t Timestep : 2861 \t\t Average Reward : 21.7\n",
      "Episode : 150 \t\t Timestep : 3132 \t\t Average Reward : 24.2\n",
      "Episode : 160 \t\t Timestep : 3467 \t\t Average Reward : 34.9\n",
      "==========================Best_agent has changed, the best reward is 34.9======================\n",
      "Episode : 170 \t\t Timestep : 3737 \t\t Average Reward : 28.2\n",
      "Episode : 180 \t\t Timestep : 3933 \t\t Average Reward : 20.4\n",
      "Episode : 190 \t\t Timestep : 4132 \t\t Average Reward : 18.8\n",
      "Episode : 200 \t\t Timestep : 4481 \t\t Average Reward : 35.3\n",
      "==========================Best_agent has changed, the best reward is 35.3======================\n",
      "Episode : 210 \t\t Timestep : 4741 \t\t Average Reward : 25.0\n",
      "Episode : 220 \t\t Timestep : 5208 \t\t Average Reward : 45.6\n",
      "==========================Best_agent has changed, the best reward is 45.6======================\n",
      "Episode : 230 \t\t Timestep : 5498 \t\t Average Reward : 31.2\n",
      "Episode : 240 \t\t Timestep : 5695 \t\t Average Reward : 19.7\n",
      "Episode : 250 \t\t Timestep : 6104 \t\t Average Reward : 37.6\n",
      "Episode : 260 \t\t Timestep : 6456 \t\t Average Reward : 34.2\n",
      "Episode : 270 \t\t Timestep : 7022 \t\t Average Reward : 58.9\n",
      "==========================Best_agent has changed, the best reward is 58.9======================\n",
      "Episode : 280 \t\t Timestep : 7372 \t\t Average Reward : 35.6\n",
      "Episode : 290 \t\t Timestep : 7722 \t\t Average Reward : 27.1\n",
      "Episode : 300 \t\t Timestep : 8541 \t\t Average Reward : 86.0\n",
      "==========================Best_agent has changed, the best reward is 86.0======================\n"
     ]
    }
   ],
   "source": [
    "best_2, rewards, loss1, loss2 = train(env, agent_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6cff46b9-9c63-4f34-a973-cf80fed54587",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "开始测试！\n",
      "回合：1/20，奖励：21.00\n",
      "回合：2/20，奖励：60.00\n",
      "回合：3/20，奖励：64.00\n",
      "回合：4/20，奖励：51.00\n",
      "回合：5/20，奖励：65.00\n",
      "回合：6/20，奖励：52.00\n",
      "回合：7/20，奖励：78.00\n",
      "回合：8/20，奖励：57.00\n",
      "回合：9/20，奖励：116.00\n",
      "回合：10/20，奖励：95.00\n",
      "回合：11/20，奖励：16.00\n",
      "回合：12/20，奖励：21.00\n",
      "回合：13/20，奖励：72.00\n",
      "回合：14/20，奖励：22.00\n",
      "回合：15/20，奖励：18.00\n",
      "回合：16/20，奖励：104.00\n",
      "回合：17/20，奖励：44.00\n",
      "回合：18/20，奖励：61.00\n",
      "回合：19/20，奖励：20.00\n",
      "回合：20/20，奖励：118.00\n",
      "完成测试\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'rewards': [21.0,\n",
       "  60.0,\n",
       "  64.0,\n",
       "  51.0,\n",
       "  65.0,\n",
       "  52.0,\n",
       "  78.0,\n",
       "  57.0,\n",
       "  116.0,\n",
       "  95.0,\n",
       "  16.0,\n",
       "  21.0,\n",
       "  72.0,\n",
       "  22.0,\n",
       "  18.0,\n",
       "  104.0,\n",
       "  44.0,\n",
       "  61.0,\n",
       "  20.0,\n",
       "  118.0]}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test(evl_env, best_2, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78225b03-604d-4aec-9529-712d9cee194b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Pytorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
