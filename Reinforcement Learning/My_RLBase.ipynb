{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "494fd1cb-e064-4cf9-a604-a52f25219579",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym # 导入gym模块\n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import math\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Categorical       # 离散概率分布 \n",
    "\n",
    "from collections import deque                     # 标准库中的一个类，它实现双端队列，允许从两端进行插入和删除操作，而不需要移动元素。\n",
    "import numpy as np\n",
    "\n",
    "import imageio                                  # imageio 是一个用于读写图像和视频的 Python 库\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "\n",
    "    ##########\n",
    "baseline = 0\n",
    "losses = []\n",
    "##########"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae42caa5-86ff-40dc-8bb9-207fcdbf763f",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### REINFORCE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4d1dbab8-7df1-4d8b-9abd-1d117c428518",
   "metadata": {},
   "outputs": [],
   "source": [
    "class REINFORCE(nn.Module):\n",
    "    def __init__(self, s_size, a_size, h_size,optimizer_type = 'Adam', lr = 1e-4, env, n_training_episodes, max_t, gamma, print_every, std_op = False):       \n",
    "        super(REINFORCE, self).__init__()                # 使子类Policy继承神经网络父类的方法和属性\n",
    "        ## Creating Layers\n",
    "        # Hidden Layers\n",
    "        self.fc1 = nn.Linear(s_size, h_size)\n",
    "        self.fc2 = nn.Linear(h_size, h_size*2)\n",
    "        # Output Layer\n",
    "        self.fc3 = nn.Linear(h_size*2, a_size)\n",
    "        self.lr = lr\n",
    "        self.env = env\n",
    "        self.n_training_episodes = n_training_episodes\n",
    "        self.max_t = max_t\n",
    "        self.gamma = gamma\n",
    "        self.print_every = print_every\n",
    "        self.std_op = False\n",
    "        \n",
    "        # 根据 optimizer_type 选择优化器\n",
    "        if optimizer_type == 'Adam':\n",
    "            self.optimizer = optim.Adam(self.parameters(), lr = self.lr)\n",
    "        elif optimizer_type == 'SGD':\n",
    "            self.optimizer = optim.SGD(self.parameters(), lr = self.lr)\n",
    "        else:\n",
    "            raise ValueError(\"Invalid optimizer_type. Supported types: 'Adam', 'SGD'.\")\n",
    "\n",
    "    # Define the Forward Pass\n",
    "    def forward(self, x):\n",
    "        # apply relu activation function to the hidden layers\n",
    "        x = F.relu(self.fc1(x))                       # 负值直接为0，可以使稀疏后的模型能够更好地挖掘相关特征，加速学习\n",
    "        x = F.relu(self.fc2(x))\n",
    "        # output with softmax\n",
    "        x = F.softmax(self.fc3(x), dim = 1)           # 归一化指数函数，多分类问题必用； 这里必须是二维输入； dim = 1表示在行维度上使用softmax   , dim=1;\n",
    "        return x                                      # x[0] 就包含第一个样本在所有类别上的概率分布\n",
    "\n",
    "    \n",
    "    # Define the act i.e. given a state, take action\n",
    "    def act(self, state):\n",
    "        state = torch.from_numpy(state).float().unsqueeze(0).to(device)   #from_numpy   pytorch搭建的网络不接受一维张量输入,unsqueeze(0)增加一个维度;  因为Net想要的是一批数据，一批自然是两维，但如果只是一个的话就需要扩展维度。 \n",
    "        probs = self.forward(state).cpu()                                 # 模型的计算可能在 GPU 上进行，将结果移回到 CPU 上进行后续处理或者与其他 CPU 上的数据进行交互。\n",
    "        m = Categorical(probs)\n",
    "        action = m.sample()                                   # 从每个动作的概率抽取，返回一个值\n",
    "        return action.item(), m.log_prob(action)              # 将抽取的动作及其对数概率作为结果返回; action.item()从tensor转换为int标量值\n",
    "\n",
    "    def reinforce(self):\n",
    "        # Help us to calculate the score during the training\n",
    "        scores_deque = deque(maxlen=100)                        # 创建长度为100的双端队列\n",
    "        scores = []\n",
    "    \n",
    "        for i_episode in range(1, self.n_training_episodes+1):\n",
    "            saved_log_probs = []\n",
    "            rewards = []\n",
    "            state = self.env.reset()[0]\n",
    "            for t in range(self.max_t):\n",
    "                action, log_prob = self.act(state)\n",
    "                saved_log_probs.append(log_prob)                       # 储存所有的对数概率   \n",
    "                state, reward, terminated, truncated, info = self.env.step(action)\n",
    "                rewards.append(reward)                                 # 把每步的reward都存起来了\n",
    "                if terminated or truncated:\n",
    "                    break\n",
    "           # if t==max_t-1:\n",
    "           #     rewards = rewards*100000\n",
    "            scores_deque.append(sum(rewards))                          # 把这次的累计得分存储起来\n",
    "            scores.append(sum(rewards))                                \n",
    "    \n",
    "            returns = deque(maxlen=self.max_t) \n",
    "            n_steps = len(rewards)                                    # 这个rewards的长度应该和max_t不一致，因为可能会terminated\n",
    "            \n",
    "            for t in range(n_steps)[::-1]:\n",
    "                 disc_return_t = (returns[0] if len(returns)>0 else 0)     # 要把初始Reward(t=maxt+1)设为0，这样的话Return(t=max)时才是正常的，只等于自己的reward，动态规划才能正常开始.\n",
    "                 returns.appendleft(self.gamma*disc_return_t + rewards[t]   )  # 动态规划了，计算每个时刻t下的回报。 这里是从后往前算; 从前插入，先进的在后面;\n",
    "                                                                           # 因为如果正着算的话很繁琐，r+y(rt+1)+...\n",
    "            if std_op == True:\n",
    "                # standardizing returns to make traininig more stable       # 将回报标准化，这样就会有正有负，加快收敛。（问题是有时已经最好了，还会去削减一些动作。）\n",
    "                eps = np.finfo(np.float32).eps.item()                       # smallest representable float  获取 np.float32 类型的最小正浮点数\n",
    "                returns = torch.tensor(returns)\n",
    "                returns = (returns - returns.mean()) / (returns.std()+eps)    # added to std deviation to avoid numerical instabilities\n",
    "    \n",
    "            policy_loss = []\n",
    "            for log_prob, disc_return in zip(saved_log_probs, returns):\n",
    "                policy_loss.append(-log_prob * (disc_return - baseline))               # REINFORCE的损失函数    # 原文是 -log_prob * disc_return，因为默认梯度下降为“-”，加上负号就是梯度上升。   当输出负的回报时，就会导致梯度还是往下走，导致出现该动作的可能降低。 \n",
    "                                                                         \n",
    "            losses = []\n",
    "            policy_loss = torch.cat(policy_loss).sum()                    # 求和，作为一个epoch的结果。\n",
    "    \n",
    "            self.optimizer.zero_grad()\n",
    "            policy_loss.backward()\n",
    "            self.optimizer.step()\n",
    "            ########\n",
    "            losses.append(policy_loss.item())\n",
    "            ########\n",
    "            if i_episode % (self.print_every) == 0:       # 就是每10轮打印一次\n",
    "                print(f\"Episode: {i_episode}\\tAverage Score: {scores[-1]}\\tPolicy Loss: {policy_loss.item()}\")   # 打印第几轮训练，平均得分，以及损失\n",
    "            # print(f\"Episode: {i_episode}\\tScore: {scores_deque[-1]}\\tPolicy Loss: {policy_loss.item()}\")   # 打印第几轮训练，平均得分，以及损失\n",
    "            \n",
    "        return scores, policy_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ea819bb-82ee-406c-ad93-764e57105e10",
   "metadata": {},
   "source": [
    "### Q-learing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "047dc66c-b0a6-475f-b19d-aa3dd9823181",
   "metadata": {},
   "outputs": [],
   "source": [
    "class REINFORCE(nn.Module):\n",
    "    def __init__(self, s_size, a_size, h_size, env, epochs, max_steps, gamma = 0.9, print_every = 10,optimizer_type = 'Adam', lr = 1e-4, std_op = False):       \n",
    "        super(REINFORCE, self).__init__()                # 使子类Policy继承神经网络父类的方法和属性\n",
    "        ## Creating Layers\n",
    "        # Hidden Layers\n",
    "        self.fc1 = nn.Linear(s_size, h_size)\n",
    "        self.fc2 = nn.Linear(h_size, h_size*2)\n",
    "        # Output Layer\n",
    "        self.fc3 = nn.Linear(h_size*2, a_size)\n",
    "        self.lr = lr\n",
    "        self.env = env\n",
    "        self.epochs = epochs\n",
    "        self.max_steps = max_steps\n",
    "        self.gamma = gamma\n",
    "        self.print_every = print_every\n",
    "        self.std_op = False\n",
    "        \n",
    "        # 根据 optimizer_type 选择优化器\n",
    "        if optimizer_type == 'Adam':\n",
    "            self.optimizer = optim.Adam(self.parameters(), lr = self.lr)\n",
    "        elif optimizer_type == 'SGD':\n",
    "            self.optimizer = optim.SGD(self.parameters(), lr = self.lr)\n",
    "        else:\n",
    "            raise ValueError(\"Invalid optimizer_type. Supported types: 'Adam', 'SGD'.\")\n",
    "\n",
    "    # Define the Forward Pass\n",
    "    def forward(self, x):\n",
    "        # apply relu activation function to the hidden layers\n",
    "        x = F.relu(self.fc1(x))                       # 负值直接为0，可以使稀疏后的模型能够更好地挖掘相关特征，加速学习\n",
    "        x = F.relu(self.fc2(x))\n",
    "        # output with softmax\n",
    "        x = F.softmax(self.fc3(x), dim = 1)           # 归一化指数函数，多分类问题必用； 这里必须是二维输入； dim = 1表示在行维度上使用softmax   , dim=1;\n",
    "        return x                                      # x[0] 就包含第一个样本在所有类别上的概率分布\n",
    "\n",
    "    \n",
    "    # Define the act i.e. given a state, take action\n",
    "    def act(self, state):\n",
    "        state = torch.from_numpy(state).float().unsqueeze(0).to(device)   #from_numpy   pytorch搭建的网络不接受一维张量输入,unsqueeze(0)增加一个维度;  因为Net想要的是一批数据，一批自然是两维，但如果只是一个的话就需要扩展维度。 \n",
    "        probs = self.forward(state).cpu()                                 # 模型的计算可能在 GPU 上进行，将结果移回到 CPU 上进行后续处理或者与其他 CPU 上的数据进行交互。\n",
    "        m = Categorical(probs)\n",
    "        action = m.sample()                                   # 从每个动作的概率抽取，返回一个值\n",
    "        return action.item(), m.log_prob(action)              # 将抽取的动作及其对数概率作为结果返回; action.item()从tensor转换为int标量值\n",
    "\n",
    "    def train(self):\n",
    "        # Help us to calculate the score during the training\n",
    "        scores_deque = deque(maxlen=100)                        # 创建长度为100的双端队列\n",
    "        scores = []\n",
    "        losses = []\n",
    "        \n",
    "        for i_episode in range(1, self.epochs+1):\n",
    "            saved_log_probs = []\n",
    "            rewards = []\n",
    "            state = self.env.reset()[0]\n",
    "            for t in range(self.max_steps):\n",
    "                action, log_prob = self.act(state)\n",
    "                saved_log_probs.append(log_prob)                       # 储存所有的对数概率   \n",
    "                state, reward, terminated, truncated, info = self.env.step(action)\n",
    "                rewards.append(reward)                                 # 把每步的reward都存起来了\n",
    "                if terminated or truncated:\n",
    "                    break\n",
    "           # if t==max_steps-1:\n",
    "           #     rewards = rewards*100000\n",
    "            scores_deque.append(sum(rewards))                          # 把这次的累计得分存储起来\n",
    "            scores.append(sum(rewards))                                \n",
    "    \n",
    "            returns = deque(maxlen=self.max_steps) \n",
    "            n_steps = len(rewards)                                    # 这个rewards的长度应该和max_steps不一致，因为可能会terminated\n",
    "            \n",
    "            for t in range(n_steps)[::-1]:\n",
    "                 disc_return_t = (returns[0] if len(returns)>0 else 0)     # 要把初始Reward(t=maxt+1)设为0，这样的话Return(t=max)时才是正常的，只等于自己的reward，动态规划才能正常开始.\n",
    "                 returns.appendleft(self.gamma*disc_return_t + rewards[t]   )  # 动态规划了，计算每个时刻t下的回报。 这里是从后往前算; 从前插入，先进的在后面;\n",
    "                                                                           # 因为如果正着算的话很繁琐，r+y(rt+1)+...\n",
    "            if self.std_op == True:\n",
    "                # standardizing returns to make traininig more stable       # 将回报标准化，这样就会有正有负，加快收敛。（问题是有时已经最好了，还会去削减一些动作。）\n",
    "                eps = np.finfo(np.float32).eps.item()                       # smallest representable float  获取 np.float32 类型的最小正浮点数\n",
    "                returns = torch.tensor(returns)\n",
    "                returns = (returns - returns.mean()) / (returns.std()+eps)    # added to std deviation to avoid numerical instabilities\n",
    "    \n",
    "            policy_loss = []\n",
    "            for log_prob, disc_return in zip(saved_log_probs, returns):\n",
    "                policy_loss.append(-log_prob * (disc_return - baseline))               # REINFORCE的损失函数    # 原文是 -log_prob * disc_return，因为默认梯度下降为“-”，加上负号就是梯度上升。   当输出负的回报时，就会导致梯度还是往下走，导致出现该动作的可能降低。 \n",
    "                                                                         \n",
    "            policy_loss = torch.cat(policy_loss).sum()                    # 求和，作为一个epoch的结果。\n",
    "    \n",
    "            self.optimizer.zero_grad()\n",
    "            policy_loss.backward()\n",
    "            self.optimizer.step()\n",
    "            ########\n",
    "            losses.append(policy_loss.item())\n",
    "            ########\n",
    "            if i_episode % (self.print_every) == 0:       # 就是每10轮打印一次\n",
    "                print(f\"Episode: {i_episode}\\tAverage Score: {scores[-1]}\\tPolicy Loss: {policy_loss.item()}\")   # 打印第几轮训练，平均得分，以及损失\n",
    "            # print(f\"Episode: {i_episode}\\tScore: {scores_deque[-1]}\\tPolicy Loss: {policy_loss.item()}\")   # 打印第几轮训练，平均得分，以及损失\n",
    "            \n",
    "        return scores, losses"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Pytorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
